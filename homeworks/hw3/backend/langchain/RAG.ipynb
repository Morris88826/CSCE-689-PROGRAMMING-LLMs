{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(DATA_PATH):\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    return loader.load()\n",
    "\n",
    "def split_text(documents, chunk_size=1000, chunk_overlap=500, verbose=True):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    if verbose:\n",
    "        print(f\"Splitting {len(documents)} documents into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks, chroma_path, verbose=True):\n",
    "    if os.path.exists(chroma_path):\n",
    "        shutil.rmtree(chroma_path)\n",
    "\n",
    "    db = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=chroma_path)\n",
    "    if verbose:\n",
    "        print(f\"Saved {len(chunks)} chunks to {chroma_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "documents = load_documents(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 1 documents into 21 chunks\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 21 chunks to chroma\n"
     ]
    }
   ],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "save_to_chroma(chunks, CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"pairwise_embedding_distance\") # calculate pairwise embedding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.13560089656702645} {'score': 0.1712324076293733} {'score': 3.3306690738754696e-16} {'score': 0.09711195935740158}\n"
     ]
    }
   ],
   "source": [
    "x = evaluator.evaluate_string_pairs(prediction=\"apple\", prediction_b=\"orange\")\n",
    "y = evaluator.evaluate_string_pairs(prediction=\"apple\", prediction_b=\"car\")\n",
    "z = evaluator.evaluate_string_pairs(prediction=\"apple\", prediction_b=\"apple\")\n",
    "k = evaluator.evaluate_string_pairs(prediction=\"apple\", prediction_b=\"iphone\")\n",
    "print(x, y, z, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_information(query: str, db: Chroma, top_k=3):\n",
    "    results = db.similarity_search_with_relevance_scores(query, k=top_k)    \n",
    "    if len(results) == 0 or results[0][1] < 0.7: # if the most similar document has a similarity score less than 0.7\n",
    "        return None\n",
    "    \n",
    "    context_text = \"\\n\\n=====\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _ in results]\n",
    "    return context_text, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to add a code section in markdown?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text, sources = retrieve_information(prompt, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the proper response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based on only the following context:\n",
    "{context}\n",
    "=====\n",
    "\n",
    "Answer the question based on the above context: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(name=\"gpt-4o-mini\", temperature=0, max_tokens=256)\n",
    "chain = RunnableSequence(prompt_template | llm | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = chain.invoke({\"query\": prompt, \"context\": context_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: To add a code section in markdown, you can use three backticks ``` before and after the code block.\n",
      "Sources: ['data/sample.md', 'data/sample.md', 'data/sample.md']\n"
     ]
    }
   ],
   "source": [
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
