# config.yaml
name: best  # name of the experiment

# File system input/output settings
input_bin: "./dev/data/fineweb10B/fineweb_train_*.bin"
input_val_bin: "./dev/data/fineweb10B/fineweb_val_*.bin"
output_dir: "./results"  # directory to save the model

# Model configuration
model: "d24"  # options: gpt2 | gpt2-medium | gpt2-large | gpt2-xl | d12 | d24 | d36 | d48

save_every: 2500 # save the model every this many iterations

# Training settings
batch_size: 8
sequence_length: 1024
total_batch_size: 524288 # total number of tokens in a batch (~0.5M)
num_iterations: 76294
inference_only: 0  # 0 for training, 1 for inference only

# Optimization settings
learning_rate: 0.0012
warmup_iters: 715
learning_rate_decay_frac: 0.1
weight_decay: 0.01
grad_clip: 1.0

# Evaluation settings
val_loss_every: 250
val_max_steps: 20
sample_every: 0

# Debugging settings
overfit_single_batch: 0  # overfit a single batch of data for testing

# Numerics settings
tensorcores: 1  # set 1 to use tensor cores
device: ""  # autodetect, or manually specify a device
compile: 1  # set to 1 to use torch.compile
flash: 1  # set to 1 to use flash attention
dtype: "bfloat16"  # options: float32 | float16 | bfloat16
zero_stage: 0  # options: 0 | 1 | 2 | 3 for zero redundancy optimizer stage

# Python -> C bridge
write_tensors: 1  # write tensors to disk

# Additional arguments
norm_method: "rmsnorm"  # options: layernorm | rmsnorm
act_method: "swiglu"  # options: gelu | swiglu
use_RoPE: 1  # set to 1 to use ROPE
group_size: 2  # set to 1 to use group size