{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements Rotary Positional Embeddings (RoPE)\n",
    "    proposed in https://arxiv.org/abs/2104.09864.\n",
    "\n",
    "    Reference implementation (used for correctness verfication)\n",
    "    can be found here:\n",
    "    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n",
    "\n",
    "    In this implementation we cache the embeddings for each position upto\n",
    "    ``max_seq_len`` by computing this during init.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Embedding dimension. This is usually set to the dim of each\n",
    "            head in the attention module computed as ````embed_dim`` // ``num_heads````\n",
    "        max_seq_len (int): Maximum expected sequence length for the\n",
    "            model, if exceeded the cached freqs will be recomputed\n",
    "        base (int): The base for the geometric progression used to compute\n",
    "            the rotation angles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        max_seq_len: int = 4096,\n",
    "        base: int = 10000,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self._rope_init()\n",
    "\n",
    "    # We need to explicitly define reset_parameters for FSDP initialization, see\n",
    "    # https://github.com/pytorch/pytorch/blob/797d4fbdf423dd9320ebe383fb57ffb1135c4a99/torch/distributed/fsdp/_init_utils.py#L885\n",
    "    def reset_parameters(self):\n",
    "        self._rope_init()\n",
    "\n",
    "    def _rope_init(self):\n",
    "        theta = 1.0 / (\n",
    "            self.base\n",
    "            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"theta\", theta, persistent=False)\n",
    "        self.build_rope_cache(self.max_seq_len)\n",
    "\n",
    "    def build_rope_cache(self, max_seq_len: int = 4096) -> None:\n",
    "        # Create position indexes `[0, 1, ..., max_seq_len - 1]`\n",
    "        seq_idx = torch.arange(\n",
    "            max_seq_len, dtype=self.theta.dtype, device=self.theta.device\n",
    "        )\n",
    "\n",
    "        # Outer product of theta and position index; output tensor has\n",
    "        # a shape of [max_seq_len, dim // 2]\n",
    "        idx_theta = torch.einsum(\"i, j -> ij\", seq_idx, self.theta).float() # outer product: ix1 * 1xj\n",
    "        \n",
    "        # cache includes both the cos and sin components and so the output shape is\n",
    "        # [max_seq_len, dim // 2, 2]\n",
    "        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "        self.register_buffer(\"cache\", cache, persistent=False)\n",
    "\n",
    "        # cache[m, k, 0] = cos(m * theta[k])\n",
    "        # cache[m, k, 1] = sin(m * theta[k])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input tensor with shape\n",
    "                [b, s, n_h, h_d]\n",
    "        Returns:\n",
    "            Tensor: output tensor with RoPE applied\n",
    "\n",
    "        Notation used for tensor shapes:\n",
    "            - b: batch size\n",
    "            - s: sequence length\n",
    "            - n_h: num heads\n",
    "            - h_d: head dim\n",
    "        \"\"\"\n",
    "        # input tensor has shape [b, s, n_h, h_d]\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # extract the values based on whether input_pos is set or not\n",
    "        rope_cache = (\n",
    "            self.cache[:seq_len]\n",
    "        )\n",
    "\n",
    "        # reshape input; the last dimension is used for computing the output.\n",
    "        # Cast to float to match the reference implementation\n",
    "        # tensor has shape [b, s, n_h, h_d // 2, 2]\n",
    "        xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "\n",
    "        # reshape the cache for broadcasting\n",
    "        # tensor has shape [b, s, 1, h_d // 2, 2] if packed samples,\n",
    "        # otherwise has shape [1, s, 1, h_d // 2, 2]\n",
    "        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2) # expand the batch and head dimensions\n",
    "\n",
    "        # tensor has shape [b, s, n_h, h_d // 2, 2]\n",
    "        x_out = torch.stack(\n",
    "            [\n",
    "                xshaped[..., 0] * rope_cache[..., 0]\n",
    "                - xshaped[..., 1] * rope_cache[..., 1],\n",
    "                xshaped[..., 1] * rope_cache[..., 0]\n",
    "                + xshaped[..., 0] * rope_cache[..., 1],\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        # tensor has shape [b, s, n_h, h_d]\n",
    "        x_out = x_out.flatten(3)\n",
    "        return x_out.type_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_embd: 768, n_head: 12, max_seq_len: 1024, head_dim: 64\n"
     ]
    }
   ],
   "source": [
    "n_embd = 768\n",
    "n_head = 12\n",
    "max_seq_len = 1024\n",
    "head_dim = n_embd // n_head\n",
    "print(f\"n_embd: {n_embd}, n_head: {n_head}, max_seq_len: {max_seq_len}, head_dim: {head_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024]) torch.Size([32])\n",
      "torch.Size([1024, 32])\n"
     ]
    }
   ],
   "source": [
    "rope = RotaryPositionalEmbeddings(head_dim, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rope.theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
